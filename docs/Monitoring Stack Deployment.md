# Monitoring stack deployment

### Preparations

Login to cloud
```
ibmcloud login --apikey ************
```

The deployment is done on Dev cluster. Login to cluster
```
ibmcloud ks cluster config --cluster c0hhi59d0s2ho34b3s00
```

Create namespace for monitoring
```
kubectl create namespace monitoring
```

Install helm
```
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
rm get_helm.sh
```

---
### Prometheus Stack Deployment

Installing Prometheus stack\
https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack

Add prometheus repo
```
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

Export file with helm chart configuration
```
mkdir monitoring
helm show values prometheus-community/kube-prometheus-stack > monitoring/prometheus-stack-conf.yml
```

Add the following
```
grafana:
  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    type: pvc
    enabled: true
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
    existingClaim: grafana
  
  ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment
  ## This can be useful for auth tokens, etc. Value is templated.
  envFromSecret: ldap-bind-password
  
  ## Grafana's primary configuration
  ## NOTE: values in map will be converted to ini format
  ## ref: http://docs.grafana.org/installation/configuration/
  ##  
  grafana.ini:
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
      filters: ldap:debug
    grafana_net:
      url: https://grafana.net
    server:
      domain: airflow.map-mktsys-dev.limited-use.ibm.com
      root_url: %(protocol)s://%(domain)s:%(http_port)s/grafana/
      serve_from_sub_path: true
    security:
      disable_initial_admin_creation: true
    auth.basic:
      enabled: false
    auth.ldap:
      enabled: true
      allow_sign_up: true
      config_file: /etc/grafana/ldap.toml

  ## Grafana's LDAP configuration
  ## Templated by the template in _helpers.tpl
  ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap
  ## ref: http://docs.grafana.org/installation/ldap/#configuration
  ldap:
    enabled: true
    # `existingSecret` is a reference to an existing secret containing the ldap configuration
    # for Grafana in a key `ldap-toml`.
    existingSecret: ""
    # `config` is the content of `ldap.toml` that will be stored in the created secret
    config: |-
      [[servers]]
      host = "bluepages.ibm.com"
      port = 636
      use_ssl = true
      start_tls = false
      ssl_skip_verify = true
      bind_dn = "uid=C-TKFU897,c=us,ou=bluepages,o=ibm.com"
      bind_password = "${LDAP_BIND_PASSWORD}"
      search_filter = "(preferredIdentity=%s)"
      search_base_dns = ["ou=bluepages,o=ibm.com"]
      group_search_filter = "(&(objectClass=ibm-nestedGroup)(uniquemember=%s))"
      group_search_filter_user_attribute = "dn"
      group_search_base_dns = ["ou=ibmgroups,o=ibm.com"]
      [servers.attributes]
      name = "cn"
      # surname = "sn"
      # member_of = "memberOf"
      email =  "mail"
      username = "mail"
      [[servers.group_mappings]]
      group_dn = "cn=CIO_Dyna_BAI-000009_PROD,ou=memberlist,ou=ibmgroups,o=ibm.com"
      org_role = "Admin"
      grafana_admin = true
      [[servers.group_mappings]]
      group_dn = "*"
      org_role = "Viewer"

prometheus:	  
  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:  
    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ##
	storageSpec:	
	## Using PersistentVolumeClaim
    ##
	  volumeClaimTemplate:
        spec:
          storageClassName: ibmc-file-gold
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi	
			  
	## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
    ## as specified in the official Prometheus documentation:
    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
    ## scrape configs are going to break Prometheus after the upgrade.
    ##
    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
    ##
	additionalScrapeConfigs:
      - job_name: 'airflow_dev'
        static_configs:
          - targets: ['statsd-exporter-prometheus-statsd-exporter:9102']
      - job_name: 'airflow_test'
        static_configs:
          - targets: ['10.221.175.106:9102']
      - job_name: 'airflow_prod'
        static_configs:
          - targets: ['10.38.78.139:9102']	    
      - job_name: 'query_exporter'
        static_configs:
          - targets: ['query-exporter:9560']	  
		  
	## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
    ## (permissions, dir tree) on mounted volumes before starting prometheus		  
    initContainers:
    - name: init-chown-data
      image: 'busybox:1.31.1'
      command:
        - chown
        - '-R'
        - '1000:2000'
        - /mnt
      resources: {}
      volumeMounts:
        - name: prometheus-prometheus-stack-kube-prom-prometheus-db
          mountPath: /mnt
      imagePullPolicy: IfNotPresent
      securityContext:
        runAsUser: 0
        runAsNonRoot: false
```
		  
Do preparations: deploy External Secret, Ingress and PVC for Grafana\
PVC for Prometheus will be deployed automatically by Helm
```
kubectl apply -f monitoring/grafana_prep.yml
kubectl get pvc grafana -n monitoring
```
You can view any UI (i.e. Prometheus) on localhost via Lens app without exposing it with Ingress or LoadBalancer

Install prometheus-stack
```
helm install prometheus-stack -f monitoring/prometheus-stack-conf.yml prometheus-community/kube-prometheus-stack -n monitoring
```

Login to Grafana with your W3ID\
There is LDAP group mapping to Dyntrace BlueGroups\
Import Dashboards on the 1st run\
Dashboars are located here
```
monitoring/cluster-dashboard.json
monitoring/dag-dashboard.json
```

---
### StatsD Exporter Deployment

Installing statsd-exporter with Helm\
https://github.com/hahow/prometheus-statsd-exporter

Add helm repo
```
helm repo add hahow https://hahow-helm-charts.storage.googleapis.com/
```

Create file with metrics mappings
```
nano monitoring/statsd_mapping.yml
```

Take data from here\
https://github.com/databand-ai/airflow-dashboards/blob/main/statsd/statsd.conf

Install statsd-exporter\
I'm using tag=latest because by default this chart downloads v0.1.14 and you'll get the error "invalid metric type 'observer'"
```
helm install statsd-exporter --set image.tag=latest --set-file statsd.mappingConfig=monitoring/statsd_mapping.yml hahow/prometheus-statsd-exporter -n monitoring
```

Disable calico policy blocking traffic flow from intranet to clusters\
Make sure the policy is not redeployed with some automation\
In case network flow stopped contact MOHAMEDIH@us.ibm.com

```
calicoctl delete GlobalNetworkPolicy block-private
```

---
### Airflow Configuration

Edit airflow.cfg
```
[metrics]
statsd_on = True
statsd_host = statsd_host = statsd-exporter-prometheus-statsd-exporter.monitoring.svc.cluster.local
statsd_port = 9125
statsd_prefix = changemeprefix
```

Add StatsD to DockerfileAirflow line where airflow is being installed\
pip install 'apache-airflow[statsd]'
```
&& SLUGIFY_USES_TEXT_UNIDECODE=yes pip install --no-cache-dir "apache-airflow[crypto,postgres,jdbc,statsd]==2.1.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.1.3/constraints-3.7.txt" psycopg2-binary scp paramiko python-ldap email_validator apache-airflow-providers-cncf-kubernetes apache-airflow-providers-slack
```

Update Airflow ConfigMap with StatsD prefix
```
kubectl apply -f YML/configmaps_dev.yml -n airflow
```

Login to CR
```
ibmcloud iam oauth-tokens | sed -ne '/IAM token/s/.* //p' | podman login -u iambearer --password-stdin us.icr.io
```

Build and push the image
```
docker build -t us.icr.io/map-dev-namespace/airflow -f DockerfileAirflow .
docker push us.icr.io/map-dev-namespace/airflow:latest
```

Restart airflow deployments
```
kubectl rollout restart deployment/airflow-webserver -n airflow
kubectl rollout restart deployment/airflow-scheduler -n airflow
```

---
### Query Exporter Deployment

Copy IBM Cloud CR ImagePullSecret to "monitoring" namespace\
**Powershell**
```
$var1 = kubectl get secret all-icr-io -n default -o yaml
(echo $var1) -replace "default","monitoring" | kubectl create -n monitoring -f -
```

Modify default ServiceAccount for airflow namespace to use ImagePullSecret from above\
**Powershell**
```
$var2 = kubectl get serviceaccount default -o yaml -n monitoring
$var2 = $var2 + "imagePullSecrets:" + "- name: all-icr-io"
$var2 | Where-Object {$_ -notlike "*resourceVersion*"} | kubectl replace serviceaccount -n monitoring default -f -
```

Create Secret in SecretsManager\
**Powershell**
```
$env:SECRETS_MANAGER_URL="https://711889a9-a7fd-47a7-b66d-12c14acccd69.us-south.secrets-manager.appdomain.cloud"
ibmcloud secrets-manager secret-create --secret-type arbitrary --resources '[{\"name\":\"DEV_DB2_DSN\",\"secret_group_id\":\"e5d844cd-fc4f-6b2c-3dd0-5f393e5ae76b\",\"payload\":\"db2://mip_dyna_mond:**************@dc400977-47fc-4300-a596-e57eccef27d3.bv7c3o6d0vfhru3npds0.databases.appdomain.cloud:30140/bludb;Security=SSL;\"}]'
```

Create Configmap
```
kubectl create configmap query-exporter --from-file=monitoring/config.yaml -n monitoring
```

Create and apply YML
```
kubectl apply -f monitoring/queryexporter.yml
```

---
### DB2 DMC Deployment

DB2 Data Management Console\
https://hub.docker.com/r/ibmcom/db2console

Create PVC
```
kubectl apply -f monitoring/dmc_pvc.yml
```

Check if status is BOUND
```
kubectl get pvc db2-dmc -n monitoring
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     AGE
db2-dmc   Bound    pvc-d6913ef5-701c-4703-ba6b-81f50a065502   20Gi       RWO            ibmc-file-gold   3m6s
```

Create Secret in SecretsManager with built-in admin password\
**Powershell**
```
$env:SECRETS_MANAGER_URL="https://711889a9-a7fd-47a7-b66d-12c14acccd69.us-south.secrets-manager.appdomain.cloud"
ibmcloud secrets-manager secret-create --secret-type arbitrary --resources '[{\"name\":\"DEV_DB2_DMC_PASSWORD\",\"secret_group_id\":\"e5d844cd-fc4f-6b2c-3dd0-5f393e5ae76b\",\"payload\":\"*********\"}]'
```

Create Configmap
```
kubectl create configmap dmc-nginx --from-file=monitoring/console-server.conf -n monitoring
```

Create and apply YML
```
kubectl apply -f monitoring/dmc.yml
```

Apply in UI configuration for LDAP and E-mail relay

DB2 Data Management Console LDAP configuration (Victor Shcherbatyuk)
DB2 Data Management Console Email configuration (Victor Shcherbatyuk)

Apply configuration for DB2 keystore and restart the app

DB2 Data Management Console Repository configuration (Victor Shcherbatyuk)

---
Next step - [Add Airflow Test and Prod environments to monitorung stack](https://github.ibm.com/CIO-MAP/MAP-ETL-Framework-AirflowK8s/blob/master/docs/Add%20Airflow%20Test%20and%20Prod%20to%20Grafana%26Prometheus.md)


